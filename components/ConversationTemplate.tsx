import { useEffect, useState, useRef } from 'react';
import styles from '../styles/Conversations.module.css';
import { GoogleGenerativeAI } from "@google/generative-ai";

declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
  }
}

const API_KEY = "AIzaSyBJjYZif960Nh_FccIVcngUZcSFfPq_tgA";
const MAX_MESSAGES = 10;

type Role = 'user' | 'assistant';
type Message = { role: Role; content: string };

interface ConversationTemplateProps {
  prompt: string;
  title: string;
  emoji?: string;
}

export default function ConversationTemplate({ prompt, title, emoji = 'ğŸ’¬' }: ConversationTemplateProps) {
  const [chatSession, setChatSession] = useState<any>(null);
  const [history, setHistory] = useState<Message[]>([]);
  const [loading, setLoading] = useState(false);
  const [listening, setListening] = useState(false);
  const [voices, setVoices] = useState<SpeechSynthesisVoice[]>([]);
  const [debugMsg, setDebugMsg] = useState<string>('Initializing...');
  const chatRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    if (chatRef.current) {
      chatRef.current.scrollTop = chatRef.current.scrollHeight;
    }
  }, [history]);

  useEffect(() => {
    const loadVoices = () => {
      const availableVoices = window.speechSynthesis.getVoices();
      setVoices(availableVoices);
    };
    if (window.speechSynthesis.onvoiceschanged !== undefined) {
      window.speechSynthesis.onvoiceschanged = loadVoices;
    }
    loadVoices();
  }, []);

  useEffect(() => {
    const initGemini = async () => {
      try {
        setDebugMsg('ğŸ“¦ Initializing Gemini...');
        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
        
        const chat = await model.startChat({
          history: [{
            role: "user",
            parts: [{ text: prompt }]
          }],
          generationConfig: {
            maxOutputTokens: 100,
            temperature: 0.7,
          },
        });
        
        setChatSession(chat);
        setDebugMsg('âœ… Gemini ready!');
      } catch (error) {
        console.error('Error initializing Gemini:', error);
        setDebugMsg('âŒ Failed to initialize Gemini');
      }
    };

    initGemini();
  }, [prompt]);

  const speak = (text: string) => {
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'ru-RU';
    const voice = voices.find(v => v.lang.startsWith('ru'));
    if (voice) utterance.voice = voice;
    window.speechSynthesis.speak(utterance);
  };

  const handleRecognizedText = async (text: string) => {
    if (!chatSession || !text) {
      setDebugMsg('âš ï¸ No input or model not ready');
      return;
    }

    const updatedHistory: Message[] = [...history, { role: 'user', content: text }];
    setHistory(updatedHistory);

    setDebugMsg('ğŸ’¬ Sending to Gemini...');
    setLoading(true);
    try {
      const result = await chatSession.sendMessage(text);
      const reply = result.response.text();
      
      setHistory([...updatedHistory, { role: 'assistant', content: reply }]);
      speak(reply);
      setDebugMsg('âœ… Response complete');
    } catch (error) {
      console.error('Error from Gemini:', error);
      setDebugMsg('âŒ Error from model');
    }
    setLoading(false);
  };

  const startRecognition = () => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert('×”×“×¤×“×¤×Ÿ ×©×œ×š ×œ× ×ª×•××š ×‘×–×™×”×•×™ ×§×•×œ×™');
      setDebugMsg('âŒ SpeechRecognition not supported');
      return;
    }

    try {
      const recognition = new SpeechRecognition();
      recognition.lang = 'ru-RU';
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      const timeoutId = setTimeout(() => recognition.stop(), 8000);

      recognition.onstart = () => {
        setDebugMsg('ğŸ™ï¸ Listening...');
        setListening(true);
      };
      recognition.onend = () => {
        clearTimeout(timeoutId);
        setListening(false);
        setDebugMsg('ğŸ›‘ Recognition ended.');
      };
      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('Speech recognition error:', event.error);
        setListening(false);
        setDebugMsg(`âŒ STT error: ${event.error}`);
      };
      recognition.onresult = (event: SpeechRecognitionEvent) => {
        const text = event.results[0][0].transcript;
        setDebugMsg(`ğŸ“ Recognized: "${text}"`);
        handleRecognizedText(text);
      };

      recognition.start();
    } catch (err) {
      console.error('Error initializing speech recognition:', err);
      setDebugMsg('âŒ Error initializing STT');
    }
  };

  const startNewConversation = async () => {
    setHistory([]);
    setDebugMsg('ğŸ”„ Starting new conversation...');
    
    try {
      const genAI = new GoogleGenerativeAI(API_KEY);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
      const chat = await model.startChat({
        history: [{
          role: "user",
          parts: [{ text: prompt }]
        }],
        generationConfig: {
          maxOutputTokens: 100,
          temperature: 0.7,
        },
      });
      setChatSession(chat);
      setDebugMsg('âœ… Ready for new conversation!');
    } catch (error) {
      console.error('Error starting new conversation:', error);
      setDebugMsg('âŒ Failed to start new conversation');
    }
  };

  const isConversationEnded = history.length >= MAX_MESSAGES;

  return (
    <div className={styles.container} dir="rtl">
      <h1 className={styles.title}>{emoji} {title}</h1>
      <p className={styles.description}>
        ×œ×—×¥ ×•×“×‘×¨ ×‘×¨×•×¡×™×ª. ×”×©×™×—×” ×ª×ª× ×”×œ ×‘×¨×•×¡×™×ª
      </p>

      <div className={styles.chatBox} ref={chatRef}>
        {history.map((msg, i) => (
          <div key={i} className={msg.role === 'user' ? styles.userMsg : styles.assistantMsg}>
            <div className={styles.msgHeader}>
              <div className={styles.messageContent}>
                <div className={styles.messageText}>
                  {msg.content}
                </div>
              </div>
              {msg.role === 'assistant' && (
                <div className={styles.messageActions}>
                  <button 
                    className={styles.iconButton}
                    onClick={() => speak(msg.content)}
                    title="Replay audio"
                  >
                    ğŸ”Š
                  </button>
                </div>
              )}
            </div>
          </div>
        ))}
      </div>

      <div className={styles.controls}>
        {isConversationEnded ? (
          <>
            <div className={styles.conversationEnded}>
              ğŸ” ×”×©×™×—×” ×”×¡×ª×™×™××”. ×œ×—×¥ ×œ××˜×” ×›×“×™ ×œ×”×ª×—×™×œ ×©×™×—×” ×—×“×©×”
            </div>
            <button 
              className={styles.newConversationButton}
              onClick={startNewConversation}
            >
              ×”×ª×—×œ ×©×™×—×” ×—×“×©×”
            </button>
          </>
        ) : (
          <button
            className={`${styles.recordButton} ${listening ? styles.recording : ''}`}
            onClick={startRecognition}
            disabled={listening || loading}
          >
            {listening ? '××§×©×™×‘...' : '×œ×—×¥ ×•×“×‘×¨'}
          </button>
        )}
      </div>

      <div style={{ marginTop: '1rem', fontSize: '1rem', color: '#666' }}>
        <strong>×¡×˜×˜×•×¡:</strong> {debugMsg}
      </div>
    </div>
  );
} 